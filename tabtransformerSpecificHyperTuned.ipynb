{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Malik\\venv\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Code\\Malik\\venv\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.14.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\\\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len CSV_HEADER: 37\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    " 'Status of aneurysm_Ruptured',\n",
    " 'Diabetes',\n",
    " 'Hypertension',\n",
    " 'Heart Disease',\n",
    " 'COPD',\n",
    " 'Polycystic Kidney Disese',\n",
    " 'Family_history',\n",
    " 'Gender_Female',\n",
    " 'Gender_Male',\n",
    " 'Gender_Transgender Female / Male-to-Female',\n",
    " 'Side_Bilateral',\n",
    " 'Side_Left',\n",
    " 'Side_Right',\n",
    " 'Side_Unknown',\n",
    " 'Side_at Bifurcation',\n",
    " 'Smoking History_Current Smoker',\n",
    " 'Smoking History_Former Smoker',\n",
    " 'Smoking History_Never Smoked',\n",
    " 'diplopia_No',\n",
    " 'diplopia_Unknown',\n",
    " 'diplopia_Yes',\n",
    " 'blurred vision_No',\n",
    " 'blurred vision_Unknown',\n",
    " 'blurred vision_Yes',\n",
    " #'New_Loc_Anterior Communicating Artery',\n",
    " #'New_Loc_BA SCA',\n",
    " #'New_Loc_Basilar Tip',\n",
    " #'New_Loc_Basilar Trunk',\n",
    " #'New_Loc_Carotid Terminus',\n",
    " #'New_Loc_Cavernous Carotid',\n",
    " #'New_Loc_Distal Branch',\n",
    " #'New_Loc_Extra-Cranial: Internal Carotid',\n",
    " #'New_Loc_ICA',\n",
    " #'New_Loc_MCA',\n",
    " #'New_Loc_PCA',\n",
    " #'New_Loc_PICA',\n",
    " #'New_Loc_Paraclinoid',\n",
    " #'New_Loc_Pericallosal',\n",
    " #'New_Loc_SICA',\n",
    " #'New_Loc_Vertebral Artery',\n",
    " 'age_category_Baby Boomers',\n",
    " 'age_category_Gen - X',\n",
    " 'age_category_Gen - Y',\n",
    " 'age_category_Silent Generation',\n",
    " 'size_category_giant',\n",
    " 'size_category_large',\n",
    " 'size_category_medium',\n",
    " 'size_category_small',\n",
    " 'size_category_tiny',\n",
    " 'Ethnicity_Asian/Oriental',\n",
    " 'Ethnicity_Black/African American',\n",
    " 'Ethnicity_Native American/American Indian',\n",
    " 'Ethnicity_White/Caucasian'\n",
    "]\n",
    "\n",
    "print(\"Len CSV_HEADER: \" + str(len(CSV_HEADER)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"ACA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (456, 37)\n",
      "Test dataset shape: (98, 37)\n",
      "Val dataset shape: (98, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Status of aneurysm_Ruptured\n",
       "ruptured      326\n",
       "unruptured    326\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(f\"./data/tab_aneurysm_data_{LOCATION}.csv\", header=None, names=CSV_HEADER)\n",
    "\n",
    "data = data[1:]\n",
    "\n",
    "X = data.drop(['Status of aneurysm_Ruptured'], axis=1)\n",
    "y = data['Status of aneurysm_Ruptured']\n",
    "\n",
    "#oversample = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "#X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "# Perform the train-test split (70% train, 30% combined validation and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Further split the temporary set into validation and test sets (50% validation, 50% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the training and testing data to CSV files\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "val_data = pd.concat([y_val, X_val], axis=1)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")\n",
    "print(f\"Val dataset shape: {val_data.shape}\")\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data_An.csv\"\n",
    "test_data_file = \"test_data_An.csv\"\n",
    "val_data_file = \"val_data_An.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)\n",
    "val_data.to_csv(val_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status of aneurysm_Ruptured</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Heart Disease</th>\n",
       "      <th>COPD</th>\n",
       "      <th>Polycystic Kidney Disese</th>\n",
       "      <th>Family_history</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Gender_Transgender Female / Male-to-Female</th>\n",
       "      <th>...</th>\n",
       "      <th>age_category_Silent Generation</th>\n",
       "      <th>size_category_giant</th>\n",
       "      <th>size_category_large</th>\n",
       "      <th>size_category_medium</th>\n",
       "      <th>size_category_small</th>\n",
       "      <th>size_category_tiny</th>\n",
       "      <th>Ethnicity_Asian/Oriental</th>\n",
       "      <th>Ethnicity_Black/African American</th>\n",
       "      <th>Ethnicity_Native American/American Indian</th>\n",
       "      <th>Ethnicity_White/Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>unruptured</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>ruptured</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>unruptured</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>ruptured</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>unruptured</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Status of aneurysm_Ruptured Diabetes Hypertension Heart Disease COPD  \\\n",
       "62                   unruptured        0            1             0    0   \n",
       "423                    ruptured        0            0             0    0   \n",
       "147                  unruptured        0            0             0    1   \n",
       "617                    ruptured        0            0             0    0   \n",
       "133                  unruptured        0            0             0    0   \n",
       "\n",
       "    Polycystic Kidney Disese Family_history Gender_Female Gender_Male  \\\n",
       "62                         0              0             1           0   \n",
       "423                        0              0             0           1   \n",
       "147                        0              1             1           0   \n",
       "617                        0              0             1           0   \n",
       "133                        0              0             1           0   \n",
       "\n",
       "    Gender_Transgender Female / Male-to-Female  ...  \\\n",
       "62                                           0  ...   \n",
       "423                                          0  ...   \n",
       "147                                          0  ...   \n",
       "617                                          0  ...   \n",
       "133                                          0  ...   \n",
       "\n",
       "    age_category_Silent Generation size_category_giant size_category_large  \\\n",
       "62                               1                   0                   0   \n",
       "423                              0                   0                   0   \n",
       "147                              0                   0                   0   \n",
       "617                              0                   0                   0   \n",
       "133                              0                   0                   0   \n",
       "\n",
       "    size_category_medium size_category_small size_category_tiny  \\\n",
       "62                     0                   0                  1   \n",
       "423                    0                   1                  0   \n",
       "147                    0                   0                  1   \n",
       "617                    0                   0                  1   \n",
       "133                    0                   1                  0   \n",
       "\n",
       "    Ethnicity_Asian/Oriental Ethnicity_Black/African American  \\\n",
       "62                         0                                0   \n",
       "423                        0                                1   \n",
       "147                        0                                0   \n",
       "617                        0                                1   \n",
       "133                        0                                0   \n",
       "\n",
       "    Ethnicity_Native American/American Indian Ethnicity_White/Caucasian  \n",
       "62                                          0                         1  \n",
       "423                                         0                         0  \n",
       "147                                         0                         1  \n",
       "617                                         0                         0  \n",
       "133                                         0                         1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURE_NAMES = [\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY={}\n",
    "for feature in CSV_HEADER[1:]: #Not including Rupture Status\n",
    "    CATEGORICAL_FEATURES_WITH_VOCABULARY[feature]=sorted(list(data[feature].unique()))\n",
    "\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [\"NA\"] for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"Status of aneurysm_Ruptured\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"ruptured\", \"unruptured\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Diabetes': ['0', '1'],\n",
       " 'Hypertension': ['0', '1'],\n",
       " 'Heart Disease': ['0', '1'],\n",
       " 'COPD': ['0', '1'],\n",
       " 'Polycystic Kidney Disese': ['0', '1'],\n",
       " 'Family_history': ['0', '1'],\n",
       " 'Gender_Female': ['0', '1'],\n",
       " 'Gender_Male': ['0', '1'],\n",
       " 'Gender_Transgender Female / Male-to-Female': ['0', '1'],\n",
       " 'Side_Bilateral': ['0', '1'],\n",
       " 'Side_Left': ['0', '1'],\n",
       " 'Side_Right': ['0', '1'],\n",
       " 'Side_Unknown': ['0', '1'],\n",
       " 'Side_at Bifurcation': ['0', '1'],\n",
       " 'Smoking History_Current Smoker': ['0', '1'],\n",
       " 'Smoking History_Former Smoker': ['0', '1'],\n",
       " 'Smoking History_Never Smoked': ['0', '1'],\n",
       " 'diplopia_No': ['0', '1'],\n",
       " 'diplopia_Unknown': ['0', '1'],\n",
       " 'diplopia_Yes': ['0', '1'],\n",
       " 'blurred vision_No': ['0', '1'],\n",
       " 'blurred vision_Unknown': ['0', '1'],\n",
       " 'blurred vision_Yes': ['0', '1'],\n",
       " 'age_category_Baby Boomers': ['0', '1'],\n",
       " 'age_category_Gen - X': ['0', '1'],\n",
       " 'age_category_Gen - Y': ['0', '1'],\n",
       " 'age_category_Silent Generation': ['0', '1'],\n",
       " 'size_category_giant': ['0', '1'],\n",
       " 'size_category_large': ['0', '1'],\n",
       " 'size_category_medium': ['0', '1'],\n",
       " 'size_category_small': ['0', '1'],\n",
       " 'size_category_tiny': ['0', '1'],\n",
       " 'Ethnicity_Asian/Oriental': ['0', '1'],\n",
       " 'Ethnicity_Black/African American': ['0', '1'],\n",
       " 'Ethnicity_Native American/American Indian': ['0'],\n",
       " 'Ethnicity_White/Caucasian': ['0', '1']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_FEATURES_WITH_VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 5  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    # print(target)\n",
    "    target_index = target_label_lookup(target)\n",
    "    return features, target_index\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=False,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset_from_csv(\"train_data_An.csv\")\n",
    "#iterator = dataset.as_numpy_iterator()\n",
    "#\n",
    "#try:\n",
    "#    while True:\n",
    "#        data_batch = next(iterator)\n",
    "#        print(data_batch)\n",
    "#except StopIteration:\n",
    "#    # This exception will be raised when the iterator is exhausted\n",
    "#    print(\"No more elements in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "def negative_predictive_value(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return tn / (tn + fn + K.epsilon())\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def get_auroc(y_true, y_pred):\n",
    "    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    val_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"), get_f1,precision,recall],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=False)\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "    validation_dataset = get_dataset_from_csv(val_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy, f1,prec,rec = model.evaluate(test_dataset, verbose=0)\n",
    "    \n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test f1: {round(f1,6)}\")\n",
    "    print(f\"Test accuracy: {round(accuracy,6)}\")\n",
    "    print(f\"Test Precision: {round(prec,6 )}\")\n",
    "    print(f\"Test Recall: {round(rec,6)}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 1999857\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = run_experiment(\n",
    "#    model=baseline_model,\n",
    "#    train_data_file=train_data_file,\n",
    "#    test_data_file=test_data_file,\n",
    "#    num_epochs=NUM_EPOCHS,\n",
    "#    learning_rate=LEARNING_RATE,\n",
    "#    weight_decay=WEIGHT_DECAY,\n",
    "#    batch_size=BATCH_SIZE,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    \n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    features = layers.Flatten()(encoded_categorical_features)\n",
    "   \n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from hypertuner_training\\ACA\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier_hp(hp):\n",
    "   \n",
    "    mlp_hidden_units_factors = [2,1]\n",
    "\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.3, step=0.05)\n",
    "    #dropout_rate = 0.2\n",
    "    embedding_dims = hp.Int('embedding_dims', min_value=8, max_value=64, step=8)\n",
    "    num_transformer_blocks = hp.Int('num_transformer_blocks', min_value=2, max_value=4, step=1)\n",
    "    num_heads = hp.Int('num_heads', min_value=4, max_value=6, step=1)\n",
    "    \n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    features = layers.Flatten()(encoded_categorical_features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"), get_f1,precision,recall ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(create_tabtransformer_classifier_hp,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='hypertuner_training',\n",
    "                     project_name=f'{LOCATION}')\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',    \n",
    "                                      patience=4,    \n",
    "                                      verbose=1,    \n",
    "                                      restore_best_weights='True',\n",
    "                                      min_delta = 0.1\n",
    "                                     )\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "validation_dataset = get_dataset_from_csv(val_data_file, batch_size)\n",
    "test_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "## Run Keras Tuner\n",
    "tuner.search(train_dataset,\n",
    "             epochs=10, \n",
    "             validation_data=validation_dataset,\n",
    "             callbacks=[ES]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dropout_rate does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Code\\Malik\\tabtransformerSpecificHyperTuned.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Code/Malik/tabtransformerSpecificHyperTuned.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m best_num_heads \u001b[39m=\u001b[39m best_hps\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Code/Malik/tabtransformerSpecificHyperTuned.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m best_trans_blocks \u001b[39m=\u001b[39m best_hps\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_transformer_blocks\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Code/Malik/tabtransformerSpecificHyperTuned.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m best_dropout_rate \u001b[39m=\u001b[39m best_hps\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mdropout_rate\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Code/Malik/tabtransformerSpecificHyperTuned.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Embedding Dimensions:\u001b[39m\u001b[39m\"\u001b[39m, best_embed_dims)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Code/Malik/tabtransformerSpecificHyperTuned.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Number of Heads:\u001b[39m\u001b[39m\"\u001b[39m, best_num_heads)\n",
      "File \u001b[1;32mc:\\Code\\Malik\\venv\\Lib\\site-packages\\keras_tuner\\src\\engine\\hyperparameters\\hyperparameters.py:246\u001b[0m, in \u001b[0;36mHyperParameters.get\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m is currently inactive.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dropout_rate does not exist.'"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_embed_dims = best_hps.get('embedding_dims')\n",
    "best_num_heads = best_hps.get('num_heads')\n",
    "best_trans_blocks = best_hps.get('num_transformer_blocks')\n",
    "best_dropout_rate = best_hps.get('dropout_rate')\n",
    "\n",
    "print(\"Best Embedding Dimensions:\", best_embed_dims)\n",
    "print(\"Best Number of Heads:\", best_num_heads)\n",
    "print(\"Best Number of Transformer Blocks:\", best_trans_blocks)\n",
    "#print(\"Best Dropout Rate:\", best_dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 8s 112ms/step - loss: 0.9950 - accuracy: 0.6447 - get_f1: 0.6542 - precision: 0.6337 - recall: 0.6928 - val_loss: 0.9455 - val_accuracy: 0.5408 - val_get_f1: 0.3543 - val_precision: 0.5417 - val_recall: 0.3135\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.9850 - accuracy: 0.6645 - get_f1: 0.6615 - precision: 0.6603 - recall: 0.6790 - val_loss: 1.8754 - val_accuracy: 0.5204 - val_get_f1: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.8316 - accuracy: 0.7149 - get_f1: 0.7205 - precision: 0.7048 - recall: 0.7553 - val_loss: 0.8512 - val_accuracy: 0.5306 - val_get_f1: 0.3062 - val_precision: 0.5000 - val_recall: 0.2817\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7502 - accuracy: 0.6754 - get_f1: 0.6605 - precision: 0.6774 - recall: 0.6539 - val_loss: 0.7223 - val_accuracy: 0.5918 - val_get_f1: 0.6670 - val_precision: 0.6771 - val_recall: 0.6607\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6812 - accuracy: 0.6952 - get_f1: 0.7056 - precision: 0.7123 - recall: 0.7199 - val_loss: 1.0706 - val_accuracy: 0.5816 - val_get_f1: 0.6780 - val_precision: 0.5275 - val_recall: 0.9544\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6698 - accuracy: 0.7039 - get_f1: 0.7141 - precision: 0.6996 - recall: 0.7442 - val_loss: 0.9704 - val_accuracy: 0.6224 - val_get_f1: 0.7738 - val_precision: 0.6688 - val_recall: 0.9544\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6703 - accuracy: 0.6996 - get_f1: 0.6938 - precision: 0.6901 - recall: 0.7060 - val_loss: 1.4050 - val_accuracy: 0.5612 - val_get_f1: 0.6708 - val_precision: 0.5179 - val_recall: 0.9544\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6567 - accuracy: 0.7083 - get_f1: 0.7189 - precision: 0.7069 - recall: 0.7454 - val_loss: 0.8689 - val_accuracy: 0.6327 - val_get_f1: 0.7735 - val_precision: 0.6748 - val_recall: 0.9365\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.5463 - accuracy: 0.7303 - get_f1: 0.7373 - precision: 0.7281 - recall: 0.7618 - val_loss: 1.0074 - val_accuracy: 0.6327 - val_get_f1: 0.6949 - val_precision: 0.5563 - val_recall: 0.9365\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5953 - accuracy: 0.7325 - get_f1: 0.7424 - precision: 0.7245 - recall: 0.7728 - val_loss: 0.7906 - val_accuracy: 0.6633 - val_get_f1: 0.7765 - val_precision: 0.6947 - val_recall: 0.9008\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.5525 - accuracy: 0.7325 - get_f1: 0.7489 - precision: 0.7296 - recall: 0.7796 - val_loss: 0.7992 - val_accuracy: 0.6224 - val_get_f1: 0.6862 - val_precision: 0.5506 - val_recall: 0.9187\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5722 - accuracy: 0.7171 - get_f1: 0.7277 - precision: 0.7164 - recall: 0.7527 - val_loss: 0.9867 - val_accuracy: 0.5918 - val_get_f1: 0.6777 - val_precision: 0.5329 - val_recall: 0.9365\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5490 - accuracy: 0.7434 - get_f1: 0.7475 - precision: 0.7301 - recall: 0.7776 - val_loss: 0.7275 - val_accuracy: 0.6633 - val_get_f1: 0.7541 - val_precision: 0.7049 - val_recall: 0.8194\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.5637 - accuracy: 0.7325 - get_f1: 0.7488 - precision: 0.7520 - recall: 0.7636 - val_loss: 0.7267 - val_accuracy: 0.6735 - val_get_f1: 0.6449 - val_precision: 0.6070 - val_recall: 0.7341\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5555 - accuracy: 0.7259 - get_f1: 0.7364 - precision: 0.7228 - recall: 0.7642 - val_loss: 0.6982 - val_accuracy: 0.7041 - val_get_f1: 0.7592 - val_precision: 0.7650 - val_recall: 0.7599\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4986 - accuracy: 0.7522 - get_f1: 0.7682 - precision: 0.7627 - recall: 0.7859 - val_loss: 0.6999 - val_accuracy: 0.7245 - val_get_f1: 0.7325 - val_precision: 0.8756 - val_recall: 0.6746\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4976 - accuracy: 0.7610 - get_f1: 0.7774 - precision: 0.7556 - recall: 0.8105 - val_loss: 0.8337 - val_accuracy: 0.6633 - val_get_f1: 0.6371 - val_precision: 0.8389 - val_recall: 0.5437\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4992 - accuracy: 0.7412 - get_f1: 0.7611 - precision: 0.7169 - recall: 0.8249 - val_loss: 0.8070 - val_accuracy: 0.6633 - val_get_f1: 0.6449 - val_precision: 0.8611 - val_recall: 0.5476\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4635 - accuracy: 0.7741 - get_f1: 0.7833 - precision: 0.8029 - recall: 0.7769 - val_loss: 0.7656 - val_accuracy: 0.6531 - val_get_f1: 0.6659 - val_precision: 0.5829 - val_recall: 0.8016\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4586 - accuracy: 0.7544 - get_f1: 0.7480 - precision: 0.7434 - recall: 0.7618 - val_loss: 0.7908 - val_accuracy: 0.6633 - val_get_f1: 0.7437 - val_precision: 0.7126 - val_recall: 0.7837\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4601 - accuracy: 0.7675 - get_f1: 0.7685 - precision: 0.7499 - recall: 0.7999 - val_loss: 0.8051 - val_accuracy: 0.6531 - val_get_f1: 0.6987 - val_precision: 0.7316 - val_recall: 0.6746\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4801 - accuracy: 0.7544 - get_f1: 0.7625 - precision: 0.7334 - recall: 0.8064 - val_loss: 0.8005 - val_accuracy: 0.6531 - val_get_f1: 0.7159 - val_precision: 0.7137 - val_recall: 0.7202\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4442 - accuracy: 0.7982 - get_f1: 0.8010 - precision: 0.8052 - recall: 0.8056 - val_loss: 0.8860 - val_accuracy: 0.6020 - val_get_f1: 0.6778 - val_precision: 0.6727 - val_recall: 0.6845\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.4641 - accuracy: 0.7763 - get_f1: 0.7977 - precision: 0.7585 - recall: 0.8495 - val_loss: 0.8831 - val_accuracy: 0.6020 - val_get_f1: 0.5887 - val_precision: 0.5667 - val_recall: 0.6567\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4560 - accuracy: 0.7719 - get_f1: 0.7875 - precision: 0.7701 - recall: 0.8127 - val_loss: 0.8773 - val_accuracy: 0.6429 - val_get_f1: 0.7061 - val_precision: 0.7127 - val_recall: 0.7024\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.4112 - accuracy: 0.8048 - get_f1: 0.8145 - precision: 0.8138 - recall: 0.8311 - val_loss: 0.8512 - val_accuracy: 0.6633 - val_get_f1: 0.7037 - val_precision: 0.7452 - val_recall: 0.6746\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4007 - accuracy: 0.8114 - get_f1: 0.8223 - precision: 0.8127 - recall: 0.8409 - val_loss: 0.8422 - val_accuracy: 0.6939 - val_get_f1: 0.7439 - val_precision: 0.7576 - val_recall: 0.7381\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4076 - accuracy: 0.7917 - get_f1: 0.8037 - precision: 0.7991 - recall: 0.8226 - val_loss: 0.8679 - val_accuracy: 0.6327 - val_get_f1: 0.6479 - val_precision: 0.5698 - val_recall: 0.7877\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3990 - accuracy: 0.8004 - get_f1: 0.8114 - precision: 0.7999 - recall: 0.8311 - val_loss: 0.9072 - val_accuracy: 0.6224 - val_get_f1: 0.6378 - val_precision: 0.5614 - val_recall: 0.7659\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3979 - accuracy: 0.8092 - get_f1: 0.8199 - precision: 0.8044 - recall: 0.8426 - val_loss: 0.8974 - val_accuracy: 0.6327 - val_get_f1: 0.6639 - val_precision: 0.5681 - val_recall: 0.8234\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3912 - accuracy: 0.8048 - get_f1: 0.8153 - precision: 0.8014 - recall: 0.8404 - val_loss: 0.9431 - val_accuracy: 0.6633 - val_get_f1: 0.6853 - val_precision: 0.5834 - val_recall: 0.8512\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4110 - accuracy: 0.8092 - get_f1: 0.8230 - precision: 0.8063 - recall: 0.8566 - val_loss: 0.9215 - val_accuracy: 0.6429 - val_get_f1: 0.6722 - val_precision: 0.5724 - val_recall: 0.8373\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4039 - accuracy: 0.7675 - get_f1: 0.7786 - precision: 0.7735 - recall: 0.7942 - val_loss: 0.9490 - val_accuracy: 0.6429 - val_get_f1: 0.6784 - val_precision: 0.5651 - val_recall: 0.8651\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3599 - accuracy: 0.8355 - get_f1: 0.8397 - precision: 0.8390 - recall: 0.8454 - val_loss: 0.9068 - val_accuracy: 0.6327 - val_get_f1: 0.6524 - val_precision: 0.5722 - val_recall: 0.7837\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3787 - accuracy: 0.8333 - get_f1: 0.8455 - precision: 0.8233 - recall: 0.8751 - val_loss: 0.9426 - val_accuracy: 0.6224 - val_get_f1: 0.6340 - val_precision: 0.5657 - val_recall: 0.7480\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3844 - accuracy: 0.8004 - get_f1: 0.8072 - precision: 0.7754 - recall: 0.8531 - val_loss: 0.9467 - val_accuracy: 0.6224 - val_get_f1: 0.6365 - val_precision: 0.5683 - val_recall: 0.7520\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3656 - accuracy: 0.8465 - get_f1: 0.8489 - precision: 0.8278 - recall: 0.8765 - val_loss: 0.9339 - val_accuracy: 0.6327 - val_get_f1: 0.6280 - val_precision: 0.7609 - val_recall: 0.5575\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3874 - accuracy: 0.8114 - get_f1: 0.8108 - precision: 0.7977 - recall: 0.8335 - val_loss: 0.9677 - val_accuracy: 0.6429 - val_get_f1: 0.6555 - val_precision: 0.5781 - val_recall: 0.7837\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3321 - accuracy: 0.8531 - get_f1: 0.8526 - precision: 0.8220 - recall: 0.8920 - val_loss: 0.9373 - val_accuracy: 0.6122 - val_get_f1: 0.6463 - val_precision: 0.5538 - val_recall: 0.8016\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3452 - accuracy: 0.8465 - get_f1: 0.8404 - precision: 0.8593 - recall: 0.8316 - val_loss: 0.9505 - val_accuracy: 0.6224 - val_get_f1: 0.6531 - val_precision: 0.5615 - val_recall: 0.8016\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3383 - accuracy: 0.8333 - get_f1: 0.8386 - precision: 0.8413 - recall: 0.8384 - val_loss: 0.9505 - val_accuracy: 0.6122 - val_get_f1: 0.6051 - val_precision: 0.5689 - val_recall: 0.6885\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3439 - accuracy: 0.8377 - get_f1: 0.8262 - precision: 0.8056 - recall: 0.8572 - val_loss: 0.9794 - val_accuracy: 0.6122 - val_get_f1: 0.6359 - val_precision: 0.5627 - val_recall: 0.7560\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3454 - accuracy: 0.8268 - get_f1: 0.8347 - precision: 0.8296 - recall: 0.8519 - val_loss: 0.9455 - val_accuracy: 0.6327 - val_get_f1: 0.6640 - val_precision: 0.5700 - val_recall: 0.8194\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3220 - accuracy: 0.8487 - get_f1: 0.8552 - precision: 0.8465 - recall: 0.8693 - val_loss: 0.9853 - val_accuracy: 0.6224 - val_get_f1: 0.6486 - val_precision: 0.5707 - val_recall: 0.7778\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3102 - accuracy: 0.8596 - get_f1: 0.8597 - precision: 0.8672 - recall: 0.8605 - val_loss: 0.9453 - val_accuracy: 0.6531 - val_get_f1: 0.6691 - val_precision: 0.5868 - val_recall: 0.8016\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3047 - accuracy: 0.8531 - get_f1: 0.8558 - precision: 0.8379 - recall: 0.8777 - val_loss: 0.9840 - val_accuracy: 0.6735 - val_get_f1: 0.6825 - val_precision: 0.6000 - val_recall: 0.8194\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3104 - accuracy: 0.8684 - get_f1: 0.8752 - precision: 0.8685 - recall: 0.8908 - val_loss: 0.9869 - val_accuracy: 0.6633 - val_get_f1: 0.6732 - val_precision: 0.5944 - val_recall: 0.8016\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2936 - accuracy: 0.8575 - get_f1: 0.8654 - precision: 0.8533 - recall: 0.8815 - val_loss: 1.0385 - val_accuracy: 0.6633 - val_get_f1: 0.6773 - val_precision: 0.5970 - val_recall: 0.8095\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3040 - accuracy: 0.8618 - get_f1: 0.8540 - precision: 0.8608 - recall: 0.8562 - val_loss: 1.0291 - val_accuracy: 0.6531 - val_get_f1: 0.6446 - val_precision: 0.6001 - val_recall: 0.7381\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.3095 - accuracy: 0.8421 - get_f1: 0.8529 - precision: 0.8371 - recall: 0.8796 - val_loss: 1.0177 - val_accuracy: 0.6531 - val_get_f1: 0.4913 - val_precision: 0.4861 - val_recall: 0.5060\n",
      "Test accuracy: 77.55%\n",
      "Test f1: 0.582136\n",
      "Test accuracy: 0.77551\n",
      "Test Precision: 0.616247\n",
      "Test Recall: 0.555764\n"
     ]
    }
   ],
   "source": [
    "tabtransformer_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = tabtransformer_model.fit(train_dataset, epochs=50, validation_data=validation_dataset)\n",
    "\n",
    "_, accuracy, f1,prec,rec= tabtransformer_model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test f1: {round(f1,6)}\")\n",
    "print(f\"Test accuracy: {round(accuracy,6)}\")\n",
    "print(f\"Test Precision: {round(prec,6 )}\")\n",
    "print(f\"Test Recall: {round(rec,6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabtransformer_model.save(f\"tab_aneurysm_model_{LOCATION}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_TRANSFORMER_BLOCKS = 4\n",
    "#NUM_HEADS = 4\n",
    "#EMBEDDING_DIMS = 40\n",
    "#DROPOUT_RATE = 0.3\n",
    "#\n",
    "#tabtransformer_model = create_tabtransformer_classifier(\n",
    "#    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "#    num_heads=NUM_HEADS,\n",
    "#    embedding_dims=EMBEDDING_DIMS,\n",
    "#    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#    dropout_rate=DROPOUT_RATE,\n",
    "#)\n",
    "#\n",
    "#print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "#keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "#\n",
    "#history = run_experiment(\n",
    "#    model=tabtransformer_model,\n",
    "#    train_data_file=train_data_file,\n",
    "#    test_data_file=test_data_file,\n",
    "#    val_data_file=val_data_file,\n",
    "#    num_epochs=NUM_EPOCHS,\n",
    "#    learning_rate=LEARNING_RATE,\n",
    "#    weight_decay=WEIGHT_DECAY,\n",
    "#    batch_size=4,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "validation_dataset = get_dataset_from_csv(val_data_file, BATCH_SIZE)\n",
    "\n",
    "predictions = tabtransformer_model.predict(validation_dataset)\n",
    "\n",
    "y_true = []  # Assuming your validation dataset contains labels (ground truth)\n",
    "for _, labels in validation_dataset:\n",
    "    y_true.append(labels)\n",
    "\n",
    "y_true = tf.concat(y_true, axis=0).numpy().tolist()\n",
    "threshold = 0.5\n",
    "\n",
    "binary_predictions = (predictions >= threshold).astype(int)\n",
    "binary_predictions = binary_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_true,  predictions) #False positive (speficifity) and true positive (sensitivity) rate\n",
    "auc = metrics.roc_auc_score(y_true, predictions) \n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
